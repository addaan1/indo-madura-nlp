{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24127bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bersihkan environment lama\n",
    "!pip uninstall -y pyarrow datasets evaluate transformers\n",
    "\n",
    "# 2. Install versi STABIL & KOMPATIBEL (Sesuai Request)\n",
    "!pip install pyarrow==14.0.1 datasets==2.15.0 evaluate==0.4.1 sacrebleu sentencepiece accelerate openpyxl -q\n",
    "!pip install transformers==4.46.3 tokenizers==0.20.3 -q\n",
    "!pip install pandas numpy scikit-learn -q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b782aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "# --- KONFIGURASI SESUAI SPEK SUKSES ---\n",
    "# Model Pengganti Cendol: mt5-small (Ringan & Stabil)\n",
    "MODEL_CHECKPOINT = \"google/mt5-small\" \n",
    "\n",
    "MAX_LENGTH = 128      # 100-128 aman\n",
    "BATCH_SIZE = 4        # Batch Kecil biar aman\n",
    "GRAD_ACCUM = 4        # Akumulasi 4x (Total Batch = 16)\n",
    "EPOCHS = 5\n",
    "OUTPUT_DIR = \"/kaggle/working/model_madura_bolak_balik\" \n",
    "\n",
    "# Cek GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# --- 1. LOAD DATASET (NusaX, INMAD, Lexicon) ---\n",
    "data_total = []\n",
    "\n",
    "# Path File (Sesuai request sebelumnya)\n",
    "DIR_NUSAX   = \"/kaggle/input/nusaxdata\"\n",
    "FILE_LEXICON = \"/kaggle/input/nusaxdata/madurese.csv\"\n",
    "FILE_INMAD  = \"/kaggle/input/inmad-dataset/INMAD Dataset.csv\"\n",
    "\n",
    "print(\"üìÇ Sedang membaca data...\")\n",
    "\n",
    "# A. Load NusaX\n",
    "for f in glob.glob(f\"{DIR_NUSAX}/*.csv\"):\n",
    "    if \"madurese.csv\" not in f and any(k in f for k in ['train', 'valid', 'test']):\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            cols = [c.lower() for c in df.columns]\n",
    "            temp = pd.DataFrame()\n",
    "            if 'indonesian' in cols: temp['indo'] = df['indonesian']\n",
    "            elif 'indonesia' in cols: temp['indo'] = df['indonesia']\n",
    "            if 'madurese' in cols: temp['madura'] = df['madurese']\n",
    "            elif 'madura' in cols: temp['madura'] = df['madura']\n",
    "            \n",
    "            if not temp.empty: data_total.append(temp)\n",
    "        except: pass\n",
    "\n",
    "# B. Load Lexicon\n",
    "try:\n",
    "    if os.path.exists(FILE_LEXICON):\n",
    "        df_lex = pd.read_csv(FILE_LEXICON)\n",
    "        temp_lex = pd.DataFrame({'indo': df_lex['indonesian'], 'madura': df_lex['madurese']})\n",
    "        data_total.append(temp_lex)\n",
    "except: pass\n",
    "\n",
    "# C. Load INMAD\n",
    "try:\n",
    "    if os.path.exists(FILE_INMAD):\n",
    "        df_inmad = pd.read_csv(FILE_INMAD)\n",
    "        df_inmad.columns = [c.strip() for c in df_inmad.columns] # Hapus spasi nama kolom\n",
    "        temp_inmad = pd.DataFrame({'indo': df_inmad['Indonesia'], 'madura': df_inmad['Madura']})\n",
    "        data_total.append(temp_inmad)\n",
    "except: pass\n",
    "\n",
    "# Gabung Semua\n",
    "df_raw = pd.concat(data_total, ignore_index=True)\n",
    "\n",
    "# Bersihkan Data (Hapus yang kosong/pendek)\n",
    "df_raw['indo'] = df_raw['indo'].astype(str).str.strip()\n",
    "df_raw['madura'] = df_raw['madura'].astype(str).str.strip()\n",
    "df_raw = df_raw[(df_raw['indo'].str.len() > 2) & (df_raw['madura'].str.len() > 2)]\n",
    "df_raw = df_raw[(df_raw['indo'] != \"nan\") & (df_raw['madura'] != \"nan\")]\n",
    "\n",
    "print(f\"‚úÖ Total Data Mentah: {len(df_raw)} pasang kalimat\")\n",
    "\n",
    "# --- 2. SETUP BOLAK-BALIK (MIRRORING) ---\n",
    "# Arah 1: Indo -> Madura\n",
    "df_indo_mad = pd.DataFrame({\n",
    "    'source': df_raw['indo'],\n",
    "    'target': df_raw['madura'],\n",
    "    'prefix': \"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: \"\n",
    "})\n",
    "\n",
    "# Arah 2: Madura -> Indo\n",
    "df_mad_indo = pd.DataFrame({\n",
    "    'source': df_raw['madura'],\n",
    "    'target': df_raw['indo'],\n",
    "    'prefix': \"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: \"\n",
    "})\n",
    "\n",
    "# Gabung Jadi Satu Dataset Besar\n",
    "df_final = pd.concat([df_indo_mad, df_mad_indo], ignore_index=True)\n",
    "print(f\"‚úÖ Total Data Training (Bolak-Balik): {len(df_final)}\")\n",
    "\n",
    "# Split Data\n",
    "train_df, val_df = train_test_split(df_final, test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# --- 3. TOKENISASI ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Gabungkan Prefix + Source Text\n",
    "    inputs = [p + str(s) for p, s in zip(examples[\"prefix\"], examples[\"source\"])]\n",
    "    targets = [str(t) for t in examples[\"target\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=MAX_LENGTH, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"‚è≥ Sedang Tokenisasi...\")\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "print(\"‚úÖ Tokenisasi Selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01089d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Setup Metrics\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Ganti -100 di label agar valid\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "# 1. Reset Memori GPU\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Load Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "model.gradient_checkpointing_enable() # Hemat Memori\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 3. CONFIG TRAINING (PERSIS SPEK SUKSES)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # --- SPEK KUNCI ---\n",
    "    per_device_train_batch_size=BATCH_SIZE, # 4\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # 4\n",
    "    gradient_accumulation_steps=GRAD_ACCUM, # 4 (Total Batch 16)\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",           # Optimizer Wajib buat T5/MT5\n",
    "    learning_rate=1e-3,          # LR standar Adafactor\n",
    "    weight_decay=0.0,\n",
    "    fp16=False,                  # WAJIB FALSE (Biar gak error NaN/0.00 loss)\n",
    "    # ------------------\n",
    "    \n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer, \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(f\"üöÄ MULAI TRAINING MODEL: {MODEL_CHECKPOINT}\")\n",
    "trainer.train()\n",
    "\n",
    "# Simpan Model Final\n",
    "print(\"üíæ Menyimpan model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"‚úÖ SELESAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6b2dc",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e27aa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Memuat model...\n",
      "üìÇ Membaca data tes: D:/UNAIR/NLP/Project_Madura/dataset4/test.csv\n",
      "‚úÖ Menguji pada 100 kalimat pertama.\n",
      "üöÄ Mulai Menerjemahkan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:13<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üèÜ REAL BLEU SCORE: 18.89\n",
      "==============================\n",
      "\n",
      "üîç 5 CONTOH HASIL:\n",
      "üáÆüá© Indo  : Dekat dengan hotel saya menginap, hanya ditempuh jalan kaki, di sini banyak sekali pilihan makanannya, tempat yang luas, dan menyenangkan\n",
      "ü§ñ Model : semma' sareng hotel kaula ngenep, coma e ajh√¢l√¢n kaki, √® dinna' b√¢nnya' pilihan kakanan, kennengngan s√® lebar, b√¢n senneng\n",
      "üîë Kunci : Semmak bik hotel engkok nginep, pera' ejeleni ajelen soko, ediye bennyak sarah pelean kakananna, kenengngan se leber, ben masenneng\n",
      "--------------------\n",
      "üáÆüá© Indo  : Iya benar, dia sedang jaga warung.\n",
      "ü§ñ Model : Iya bendher, rowa teppa' jaga warung.\n",
      "üîë Kunci : Iye bhender, rua ajege berung.\n",
      "--------------------\n",
      "üáÆüá© Indo  : Kangkungnya lumayan tapi kepiting saus padangnya mengecewakan kami dikasih kepiting yang kopong akhir kami tidak makan keptingnya dan dikembalikan.\n",
      "ü§ñ Model : Kangkungna pendhenan tape kepiting saos padhangnga ngacewaaghi kami eberri' kepiting se kopong akhir kami ta' ngakan kepiting ban ekembaliaghi.\n",
      "üîë Kunci : Kangkongnga pendhanan tape kopeteng saos padangnga ma kocaba, engko' bi' laenna e bharri' kopeteng se kopong akherra engko' bi' laenna ta' ngakan kopeteng ban e pabali.\n",
      "--------------------\n",
      "üáÆüá© Indo  : Bertempat di braga city walk yang satu gedung dengan aston dan fave hotel, tempat ini sangat nyaman buat kongkow-kongkow. Kopi campur teh yang baru pertama kali saya nikmati ternyata sangat enak, dipadu dengan telur setengah matang menjadi pendamping mengobrol bersama teman-teman. Area yang bebas merokok semakin mengasyikkan sambil menikmati pemandangan lalu lalang orang-orang yang keluar masuk mal ini.\n",
      "ü§ñ Model : kennengngan e braga city walk se settong geddhung bik aston ben fave hotel, kennengngan riya nyaman ghebey kongkow-kongkow. Kopi campur teh se bhuru pertama kale engkok nikmati ternyata cek nyamanna, epadu bik tellor satengnga matang deddhi pendamping ngobrol bik ca-kanca. Area se bebas arokok sajhen ngasenneng sambih nikmati pemandangan pas lelang reng-oreng se kalowar masok mal riya.\n",
      "üîë Kunci : Kenengnganna e braga city walk se settong gheddung bik aston ben fave hotel, kenengngan riya nyaman sarah ghebey tor-catoran. Biddheng campor teh se ludhulluna engkok nginum bhuktena cek nyamanna, ecampor bik tellor massak saparo deddhi pendamping tor-catoran bik ca-kanca. Kenengngan se olle arokok tambe masenneng sambi menikmati pangabesen oreng-oreng se aje'genjir kaluar masok mal riya.\n",
      "--------------------\n",
      "üáÆüá© Indo  : Gianyar terima bantuan sosial 2018 sebesar rp 44, 9 miliar\n",
      "ü§ñ Model : Gianyar nar√®ma bh√¢ntowan sosial 2018 ra-k√®ra 44,9 milyar\n",
      "üîë Kunci : Gianyar tarema bhantoan sosial 2018 saraja rp 44,9 miliar.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è KONFIGURASI\n",
    "# ==========================================\n",
    "# 1. Path Model (Sesuai yang tadi)\n",
    "MODEL_PATH = r\"D:/UNAIR/NLP/Project_Madura/model_google_mt5\"\n",
    "\n",
    "# 2. Path File Test (Cari file test.csv dari NusaX di laptopmu)\n",
    "# Kalau tidak ada, pakai file train.csv tapi kita ambil 100 baris aja buat tes\n",
    "FILE_TEST_PATH = r\"D:/UNAIR/NLP/Project_Madura/dataset4/test.csv\"  # <--- GANTI INI DENGAN LOKASI FILE CSV KAMU!\n",
    "\n",
    "# ==========================================\n",
    "# üîß FUNGSI BERSIH-BERSIH (PENTING!)\n",
    "# ==========================================\n",
    "def bersihkan_teks(text):\n",
    "    # Buang link https://...\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Buang mention @user\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Hapus spasi berlebih\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ EVALUASI FULL\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"üìÇ Memuat model...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "    except:\n",
    "        print(\"‚ùå Model tidak ketemu. Cek path lagi.\")\n",
    "        return\n",
    "\n",
    "    # Load Data Test\n",
    "    print(f\"üìÇ Membaca data tes: {FILE_TEST_PATH}\")\n",
    "    try:\n",
    "        # Coba baca CSV. Sesuaikan nama kolomnya nanti\n",
    "        df = pd.read_csv(FILE_TEST_PATH)\n",
    "        \n",
    "        # Cari kolom indo dan madura\n",
    "        cols = [c.lower() for c in df.columns]\n",
    "        src_col, tgt_col = None, None\n",
    "        \n",
    "        if 'indonesian' in cols: src_col = 'indonesian'\n",
    "        elif 'indonesia' in cols: src_col = 'indonesia'\n",
    "        \n",
    "        if 'madurese' in cols: tgt_col = 'madurese'\n",
    "        elif 'madura' in cols: tgt_col = 'madura'\n",
    "        \n",
    "        if not src_col or not tgt_col:\n",
    "            print(\"‚ùå Kolom 'indonesian' atau 'madurese' tidak ditemukan di CSV.\")\n",
    "            print(f\"Kolom yang ada: {cols}\")\n",
    "            return\n",
    "            \n",
    "        # Ambil 100 data saja biar cepat (kalau mau semua, hapus .head(100))\n",
    "        df_sample = df.head(100) \n",
    "        print(f\"‚úÖ Menguji pada {len(df_sample)} kalimat pertama.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal baca CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    print(\"üöÄ Mulai Menerjemahkan...\")\n",
    "    \n",
    "    for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "        src_text = str(row[src_col])\n",
    "        ref_text = str(row[tgt_col])\n",
    "        \n",
    "        # Tambah Prefix\n",
    "        input_text = f\"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: {src_text}\"\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Kita atur parameter biar gak halusinasi (repetition_penalty)\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_length=128, \n",
    "                num_beams=4,\n",
    "                repetition_penalty=1.5, # Hukuman biar gak ngulang kata\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Bersihkan hasil (buang link dll)\n",
    "        pred_bersih = bersihkan_teks(pred)\n",
    "        \n",
    "        predictions.append(pred_bersih)\n",
    "        references.append([ref_text])\n",
    "\n",
    "    # Hitung Skor\n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"üèÜ REAL BLEU SCORE: {results['score']:.2f}\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Tampilkan 5 contoh hasil\n",
    "    print(\"\\nüîç 5 CONTOH HASIL:\")\n",
    "    for i in range(5):\n",
    "        print(f\"üáÆüá© Indo  : {df_sample.iloc[i][src_col]}\")\n",
    "        print(f\"ü§ñ Model : {predictions[i]}\")\n",
    "        print(f\"üîë Kunci : {references[i][0]}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b228648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Memuat model...\n",
      "üìÇ Membaca data tes: D:/UNAIR/NLP/Project_Madura/dataset4/test.csv\n",
      "‚úÖ Menguji arah MADURA -> INDONESIA pada 400 kalimat.\n",
      "üöÄ Mulai Menerjemahkan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [12:28<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üèÜ REAL BLEU SCORE (MAD -> INDO): 43.01\n",
      "==============================\n",
      "\n",
      "üîç 5 CONTOH HASIL:\n",
      "üêÇ Madura : Semmak bik hotel engkok nginep, pera' ejeleni ajelen soko, ediye bennyak sarah pelean kakananna, kenengngan se leber, ben masenneng\n",
      "ü§ñ Model  : dekat dengan hotel saya menginap, hanya dijalankan jalan kaki, di sini banyak sekali pilihan makanannya, tempat yang luas, dan menyenangkan\n",
      "üáÆüá© Kunci  : Dekat dengan hotel saya menginap, hanya ditempuh jalan kaki, di sini banyak sekali pilihan makanannya, tempat yang luas, dan menyenangkan\n",
      "--------------------\n",
      "üêÇ Madura : Iye bhender, rua ajege berung.\n",
      "ü§ñ Model  : iya benar, itu jaga warung.\n",
      "üáÆüá© Kunci  : Iya benar, dia sedang jaga warung.\n",
      "--------------------\n",
      "üêÇ Madura : Kangkongnga pendhanan tape kopeteng saos padangnga ma kocaba, engko' bi' laenna e bharri' kopeteng se kopong akherra engko' bi' laenna ta' ngakan kopeteng ban e pabali.\n",
      "ü§ñ Model  : makasih lumayan tapi kopeteng saos padangnya mengatakan, saya dan lainnya dikasih kopeteng yang kopong akhirnya saya tidak makan kopeteng dan dikembalikan.\n",
      "üáÆüá© Kunci  : Kangkungnya lumayan tapi kepiting saus padangnya mengecewakan kami dikasih kepiting yang kopong akhir kami tidak makan keptingnya dan dikembalikan.\n",
      "--------------------\n",
      "üêÇ Madura : Kenengnganna e braga city walk se settong gheddung bik aston ben fave hotel, kenengngan riya nyaman sarah ghebey tor-catoran. Biddheng campor teh se ludhulluna engkok nginum bhuktena cek nyamanna, ecampor bik tellor massak saparo deddhi pendamping tor-catoran bik ca-kanca. Kenengngan se olle arokok tambe masenneng sambi menikmati pangabesen oreng-oreng se aje'genjir kaluar masok mal riya.\n",
      "ü§ñ Model  : Tempatnya di braga city walk yang satu bangunan dengan aston dan fave hotel, tempat ini nyaman banget buat nongkrong. Bangunan campuran teh yang lucunya saya minum buktinya sangat enak, dicampor dengan lemari masak saparo jadi pendamping teman-teman. Tempat yang dapat rokok tambah menyenangkan sambil menikmati pemandangan\n",
      "üáÆüá© Kunci  : Bertempat di braga city walk yang satu gedung dengan aston dan fave hotel, tempat ini sangat nyaman buat kongkow-kongkow. Kopi campur teh yang baru pertama kali saya nikmati ternyata sangat enak, dipadu dengan telur setengah matang menjadi pendamping mengobrol bersama teman-teman. Area yang bebas merokok semakin mengasyikkan sambil menikmati pemandangan lalu lalang orang-orang yang keluar masuk mal ini.\n",
      "--------------------\n",
      "üêÇ Madura : Gianyar tarema bhantoan sosial 2018 saraja rp 44,9 miliar.\n",
      "ü§ñ Model  : Gianyar terima bantuan sosial 2018 sebesar rp 44,9 miliar.\n",
      "üáÆüá© Kunci  : Gianyar terima bantuan sosial 2018 sebesar rp 44, 9 miliar\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è KONFIGURASI\n",
    "# ==========================================\n",
    "# 1. Path Model\n",
    "MODEL_PATH = r\"D:/UNAIR/NLP/Project_Madura/model_google_mt5\"\n",
    "\n",
    "# 2. Path File Test\n",
    "FILE_TEST_PATH = r\"D:/UNAIR/NLP/Project_Madura/dataset4/test.csv\"\n",
    "\n",
    "# ==========================================\n",
    "# üîß FUNGSI BERSIH-BERSIH\n",
    "# ==========================================\n",
    "def bersihkan_teks(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ EVALUASI FULL (MADURA -> INDO)\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"üìÇ Memuat model...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "    except:\n",
    "        print(\"‚ùå Model tidak ketemu. Cek path lagi.\")\n",
    "        return\n",
    "\n",
    "    # Load Data Test\n",
    "    print(f\"üìÇ Membaca data tes: {FILE_TEST_PATH}\")\n",
    "    try:\n",
    "        df = pd.read_csv(FILE_TEST_PATH)\n",
    "        \n",
    "        # Deteksi nama kolom\n",
    "        cols = [c.lower() for c in df.columns]\n",
    "        col_indo, col_madura = None, None\n",
    "        \n",
    "        # Cari kolom Indo\n",
    "        if 'indonesian' in cols: col_indo = 'indonesian'\n",
    "        elif 'indonesia' in cols: col_indo = 'indonesia'\n",
    "        \n",
    "        # Cari kolom Madura\n",
    "        if 'madurese' in cols: col_madura = 'madurese'\n",
    "        elif 'madura' in cols: col_madura = 'madura'\n",
    "        \n",
    "        if not col_indo or not col_madura:\n",
    "            print(\"‚ùå Kolom tidak lengkap.\")\n",
    "            return\n",
    "            \n",
    "        # Ambil 100 data pertama\n",
    "        df_sample = df.head(400) \n",
    "        print(f\"‚úÖ Menguji arah MADURA -> INDONESIA pada {len(df_sample)} kalimat.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal baca CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    print(\"üöÄ Mulai Menerjemahkan...\")\n",
    "    \n",
    "    for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "        # --- PERUBAHAN UTAMA DI SINI ---\n",
    "        \n",
    "        # 1. Input sekarang Bahasa MADURA\n",
    "        src_text = str(row[col_madura]) \n",
    "        \n",
    "        # 2. Kunci Jawaban sekarang Bahasa INDONESIA\n",
    "        ref_text = str(row[col_indo])   \n",
    "        \n",
    "        # 3. Prefix DIBALIK\n",
    "        input_text = f\"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: {src_text}\"\n",
    "        # -------------------------------\n",
    "        \n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_length=128, \n",
    "                num_beams=4,\n",
    "                repetition_penalty=1.5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred_bersih = bersihkan_teks(pred)\n",
    "        \n",
    "        predictions.append(pred_bersih)\n",
    "        references.append([ref_text])\n",
    "\n",
    "    # Hitung Skor\n",
    "    results = metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"üèÜ REAL BLEU SCORE (MAD -> INDO): {results['score']:.2f}\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Tampilkan 5 contoh hasil\n",
    "    print(\"\\nüîç 5 CONTOH HASIL:\")\n",
    "    for i in range(5):\n",
    "        # Tampilkan label yang benar\n",
    "        print(f\"üêÇ Madura : {df_sample.iloc[i][col_madura]}\") \n",
    "        print(f\"ü§ñ Model  : {predictions[i]}\")\n",
    "        print(f\"üáÆüá© Kunci  : {references[i][0]}\") # Kunci Jawaban Indo\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82331cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ü§ñ MEMUAT MODEL PENERJEMAH MADURA...\n",
      "üìÇ Path: D:/UNAIR/NLP/Project_Madura/model_google_mt5\n",
      "==================================================\n",
      "‚ö° Sedang memuat ke CUDA... Mohon tunggu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI-PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model SIAP digunakan!\n",
      "\n",
      "------------------------------\n",
      "PILIH MENU:\n",
      "1. Indo -> Madura\n",
      "2. Madura -> Indo\n",
      "3. Keluar\n",
      "\n",
      "üìù Ketik kalimat yang ingin diterjemahkan:\n",
      "\n",
      "========================================\n",
      "üîÑ üáÆüá© INDO -> üêÇ MADURA\n",
      "üìù Input : halo\n",
      "ü§ñ Hasil : halo\n",
      "========================================\n",
      "\n",
      "------------------------------\n",
      "PILIH MENU:\n",
      "1. Indo -> Madura\n",
      "2. Madura -> Indo\n",
      "3. Keluar\n",
      "\n",
      "üìù Ketik kalimat yang ingin diterjemahkan:\n",
      "\n",
      "========================================\n",
      "üîÑ üáÆüá© INDO -> üêÇ MADURA\n",
      "üìù Input : halo apa kabar\n",
      "ü§ñ Hasil : halo apa kabar\n",
      "========================================\n",
      "\n",
      "------------------------------\n",
      "PILIH MENU:\n",
      "1. Indo -> Madura\n",
      "2. Madura -> Indo\n",
      "3. Keluar\n",
      "üëã Sampai jumpa!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è KONFIGURASI PATH\n",
    "# ==========================================\n",
    "MODEL_PATH = r\"D:/UNAIR/NLP/Project_Madura/model_google_mt5\"\n",
    "\n",
    "# ==========================================\n",
    "# üîß FUNGSI PEMBERSIH (Opsional)\n",
    "# ==========================================\n",
    "def bersihkan_hasil(text):\n",
    "    # Hapus spasi ganda\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # (Opsional) Hapus tanda petik jika ingin hasil polos\n",
    "    # text = re.sub(r\"['`‚Äô‚Äò]\", \"\", text) \n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ APLIKASI UTAMA\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"=\"*50)\n",
    "    print(\"ü§ñ MEMUAT MODEL PENERJEMAH MADURA...\")\n",
    "    print(f\"üìÇ Path: {MODEL_PATH}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Cek Device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"‚ö° Sedang memuat ke {device.upper()}... Mohon tunggu.\")\n",
    "\n",
    "    # 2. Load Model\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "        print(\"‚úÖ Model SIAP digunakan!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal load model: {e}\")\n",
    "        print(\"Pastikan path benar dan library protobuf sudah diinstall.\")\n",
    "        return\n",
    "\n",
    "    # 3. Loop Interaktif\n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*30)\n",
    "        print(\"PILIH MENU:\")\n",
    "        print(\"1. Indo -> Madura\")\n",
    "        print(\"2. Madura -> Indo\")\n",
    "        print(\"3. Keluar\")\n",
    "        \n",
    "        pilihan = input(\"üëâ Masukkan angka (1/2/3): \").strip()\n",
    "        \n",
    "        if pilihan == '3':\n",
    "            print(\"üëã Sampai jumpa!\")\n",
    "            break\n",
    "        \n",
    "        if pilihan not in ['1', '2']:\n",
    "            print(\"‚ö†Ô∏è Pilihan salah, coba lagi.\")\n",
    "            continue\n",
    "\n",
    "        # Minta Input Kalimat\n",
    "        print(\"\\nüìù Ketik kalimat yang ingin diterjemahkan:\")\n",
    "        kalimat = input(\"üëâ Input: \").strip()\n",
    "        \n",
    "        if not kalimat:\n",
    "            print(\"‚ö†Ô∏è Kalimat kosong.\")\n",
    "            continue\n",
    "\n",
    "        # Tentukan Prefix\n",
    "        if pilihan == '1':\n",
    "            prefix = \"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: \"\n",
    "            arah = \"üáÆüá© INDO -> üêÇ MADURA\"\n",
    "        else:\n",
    "            prefix = \"terjemahkan dari Bahasa Madura ke Bahasa Indonesia: \"\n",
    "            arah = \"üêÇ MADURA -> üáÆüá© INDO\"\n",
    "\n",
    "        # Gabung Prefix + Kalimat\n",
    "        input_text = prefix + kalimat\n",
    "        \n",
    "        # Proses Translate\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_length=128, \n",
    "                num_beams=5,            # Beam 5 biar mikir lebih cerdas\n",
    "                repetition_penalty=1.2, # Cegah kata berulang\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        hasil = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        hasil_bersih = bersihkan_hasil(hasil)\n",
    "\n",
    "        # Tampilkan Hasil\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\"üîÑ {arah}\")\n",
    "        print(f\"üìù Input : {kalimat}\")\n",
    "        print(f\"ü§ñ Hasil : {hasil_bersih}\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e1010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ MEMUAT MODEL DARI: D:/UNAIR/NLP/Project_Madura/model_google_mt5\n",
      "‚ö° Device: CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI-PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Masuk: 50 soal dari NusaX.\n",
      "‚úÖ Masuk: 50 soal dari Lexicon.\n",
      "‚úÖ Masuk: 50 soal dari INMAD.\n",
      "\n",
      "üöÄ MULAI UJIAN (150 Soal)...\n",
      "Parameter: Beam=5, Normalisasi=AKTIF (Anti-Petik)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/150 [00:04<11:36,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sosmed (NusaX)] Indo: Untuk menuju ke tempat ini jalannya sangat macet dan jauh dari pusat kota. Sesampainya di sana tidak ada hal yang spesial yang membuat perjalanan jauh terbayarkan. Menunya biasa saja.\n",
      "ü§ñ Jawab : Kaangghuy entar ka kennengngan pan√®ka jh√¢l√¢nna c√®' macet b√¢n jh√¢u d√¢ri pusat kottha. Samp√®' √® dhissa' tad√¢' hal s√® istimewa s√® agh√¢b√¢y parjh√¢l√¢nan jh√¢u √®b√¢yar. Menuna biasa sajan.  \t-> (Norm: kaangghuy entar ka kennengngan pan√®ka j√¢l√¢nna c√® macet b√¢n j√¢u d√¢ri pusat kottha samp√® √® dissa tad√¢ hal s√® istimewa s√® agh√¢b√¢y parj√¢l√¢nan j√¢u √®b√¢yar menuna biasa sajan)\n",
      "üîë Kunci : Untuk depak ka kennengngan ria jhelenna macet ben jheu deri pusat kota. Sa depak en ka dissak tadek se spesial se ghebey perjhelenan jheu ria terbayarkan. Menuna biasa bhei.      \t-> (Norm: untuk depak ka kennengngan ria jelenna macet ben jeu deri pusat kota sa depak en ka dissak tadek se spesial se ghebey perjelenan jeu ria terbayarkan menuna biasa bei)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 26/150 [00:59<05:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Sosmed (NusaX)] Indo: Banyak orang yang kurang suka untuk berobat ke dokter. Selain biaya yang relatif cukup mahal bagi sebagian orang, ke dokter tak selamanya bisa menyembuhkan. Banyak dokter yang salah diagnosa. Pentingkah ataukah sudah lumrah, toh dokter juga manusia.\n",
      "ü§ñ Jawab : B√¢nnya' or√®ng s√® ta' senneng kaangghuy aobat ka dokter. Sala√®n ongkos s√® relatif cokop larang gh√¢b√¢y sab√¢giy√¢n or√®ng, ka dokter ta' samp√®yan bisa mab√¢li. b√¢nnya' dokter s√® sala diagnosa. Pentingkah otab√¢ ampon lumrah, toh dokter jhugh√¢n manossa.  \t-> (Norm: b√¢nnya or√®ng s√® ta senneng kaangghuy aobat ka dokter sala√®n ongkos s√® relatif cokop larang gh√¢b√¢y sab√¢giy√¢n or√®ng ka dokter ta samp√®yan bisa mab√¢li b√¢nnya dokter s√® sala diagnosa pentingkah otab√¢ ampon lumrah toh dokter jugh√¢n manossa)\n",
      "üîë Kunci : Bennyak oreng se korang seneng ghebey aobat ka dokter. Salaen ongkos se bek cokop larang ghebey sabegien oreng, ka dokter tak saterrossa bisa maberes. Bennyak dokter se salah nantoaghi panyakek. Pentingkah otabe la biasa, toh dokter manussa kiya.      \t-> (Norm: bennyak oreng se korang seneng ghebey aobat ka dokter salaen ongkos se bek cokop larang ghebey sabegien oreng ka dokter tak saterrossa bisa maberes bennyak dokter se salah nantoaghi panyakek pentingkah otabe la biasa toh dokter manussa kiya)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 51/150 [02:00<02:45,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kamus (Lexicon)] Indo: mengembalikan\n",
      "ü§ñ Jawab : ngembalikan  \t-> (Norm: ngembalikan)\n",
      "üîë Kunci : mabeliagi      \t-> (Norm: mabeliagi)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 77/150 [02:05<00:10,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kamus (Lexicon)] Indo: tidak\n",
      "ü§ñ Jawab : ta'  \t-> (Norm: ta)\n",
      "üîë Kunci : √™nt√™n      \t-> (Norm: √™nt√™n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 101/150 [02:10<00:24,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Formal (INMAD)] Indo: Kau yang aku tunggu dengan Afgan oleh rossa?\n",
      "ü§ñ Jawab : Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi Budi [URL]  \t-> (Norm: budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi budi url)\n",
      "üîë Kunci : edisi posang malem minggu ? Kamu yang kutunggu ( bik Afgan) deri Rossa ? http://path.com/p/nrLjm      \t-> (Norm: edisi posang malem minggu  kamu yang kutunggu  bik afgan deri rossa)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 126/150 [02:54<00:36,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Formal (INMAD)] Indo: dia memberiku handuk kotor tidak ada wifi di dalam ruangan jangan katakan wifi!\n",
      "ü§ñ Jawab : kaul√¢h aberri' handuk gheddhe' tad√¢' wifi √® kamar jh√¢' √®kab√¢'aghi wifi!  \t-> (Norm: kaul√¢h aberri handuk ghedde tad√¢ wifi √® kamar j√¢ √®kab√¢aghi wifi)\n",
      "üîë Kunci : sempat eberri' andok gheddhe'. tade' WiFi e kamar. jhe' nyator bede WiFi!      \t-> (Norm: sempat eberri andok ghedde tade wifi e kamar je nyator bede wifi)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [03:40<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üèÜ SKOR BLEU FINAL (ADIL): 14.32\n",
      "========================================\n",
      "‚ö†Ô∏è LUMAYAN. (Perlu perbaikan gaya bahasa)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# ‚öôÔ∏è KONFIGURASI PATH (SESUAI REQUEST)\n",
    "# ==========================================\n",
    "# 1. Lokasi Model (Sesuai log error Bos sebelumnya)\n",
    "MODEL_PATH = r\"D:/UNAIR/NLP/Project_Madura/model_google_mt5\"\n",
    "\n",
    "# 2. Lokasi Dataset (Asumsi ada di folder dataset_nusax di drive D)\n",
    "# Pastikan nama file csv-nya sesuai dengan yang ada di laptop Bos\n",
    "PATH_NUSAX   = r\"D:/UNAIR/NLP/Project_Madura/dataset4/test.csv\"       # File Test NusaX\n",
    "PATH_LEXICON = r\"D:/UNAIR/NLP/Project_Madura/dataset4/madurese.csv\"   # File Kamus (Lexicon)\n",
    "PATH_INMAD   = r\"D:/UNAIR/NLP/Project_Madura/dataset3/INMAD Dataset.csv\" # File Formal INMAD\n",
    "\n",
    "# Jumlah soal per tipe data (Total soal = 3 x 50 = 150 soal)\n",
    "JUMLAH_SAMPEL_PER_DATA = 50 \n",
    "\n",
    "# ==========================================\n",
    "# üßπ FUNGSI PEMBERSIH EJAAN (JURUS RAHASIA)\n",
    "# ==========================================\n",
    "def normalisasi_madura(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 1. Buang Link & Mention sampah\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 2. HAPUS TANDA PETIK & EJAAN KHAS\n",
    "    # Ini biar 'nase`' == 'nase' == \"nase'\"\n",
    "    # Kita hapus petik satu ('), backtick (`), dan petik miring (‚Äô)\n",
    "    text = re.sub(r\"['`‚Äô‚Äò]\", \"\", text) \n",
    "    \n",
    "    # 3. Samakan Ejaan Umum (Opsional)\n",
    "    # Kadang 'dh' ditulis 'd', 'bh' ditulis 'b', 'jh' ditulis 'j'\n",
    "    text = text.replace(\"dh\", \"d\").replace(\"bh\", \"b\").replace(\"jh\", \"j\")\n",
    "    \n",
    "    # 4. Hapus tanda baca lain & spasi ganda\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ==========================================\n",
    "# üìÇ LOAD DATA MIX (Chat + Kamus + Formal)\n",
    "# ==========================================\n",
    "def load_data_ujian():\n",
    "    soal_ujian = []\n",
    "    \n",
    "    # A. NUSAX (Bahasa Kasar/Chat)\n",
    "    if os.path.exists(PATH_NUSAX):\n",
    "        try:\n",
    "            df = pd.read_csv(PATH_NUSAX)\n",
    "            cols = [c.lower() for c in df.columns]\n",
    "            src, tgt = None, None\n",
    "            if 'indonesian' in cols: src = 'indonesian'\n",
    "            elif 'indonesia' in cols: src = 'indonesia'\n",
    "            if 'madurese' in cols: tgt = 'madurese'\n",
    "            elif 'madura' in cols: tgt = 'madura'\n",
    "            \n",
    "            if src and tgt:\n",
    "                sample = df.sample(n=min(len(df), JUMLAH_SAMPEL_PER_DATA), random_state=42)\n",
    "                for _, row in sample.iterrows():\n",
    "                    soal_ujian.append({\"tipe\": \"Sosmed (NusaX)\", \"indo\": row[src], \"madura\": row[tgt]})\n",
    "                print(f\"‚úÖ Masuk: {len(sample)} soal dari NusaX.\")\n",
    "        except: print(\"‚ö†Ô∏è Gagal baca NusaX.\")\n",
    "    else:\n",
    "        print(f\"‚ùå File tidak ketemu: {PATH_NUSAX}\")\n",
    "\n",
    "    # B. LEXICON (Kamus)\n",
    "    if os.path.exists(PATH_LEXICON):\n",
    "        try:\n",
    "            df = pd.read_csv(PATH_LEXICON)\n",
    "            sample = df.sample(n=min(len(df), JUMLAH_SAMPEL_PER_DATA), random_state=42)\n",
    "            for _, row in sample.iterrows():\n",
    "                soal_ujian.append({\"tipe\": \"Kamus (Lexicon)\", \"indo\": row['indonesian'], \"madura\": row['madurese']})\n",
    "            print(f\"‚úÖ Masuk: {len(sample)} soal dari Lexicon.\")\n",
    "        except: print(\"‚ö†Ô∏è Gagal baca Lexicon.\")\n",
    "    else:\n",
    "        print(f\"‚ùå File tidak ketemu: {PATH_LEXICON}\")\n",
    "\n",
    "    # C. INMAD (Formal)\n",
    "    if os.path.exists(PATH_INMAD):\n",
    "        try:\n",
    "            df = pd.read_csv(PATH_INMAD)\n",
    "            df.columns = [c.strip() for c in df.columns]\n",
    "            sample = df.sample(n=min(len(df), JUMLAH_SAMPEL_PER_DATA), random_state=42)\n",
    "            for _, row in sample.iterrows():\n",
    "                soal_ujian.append({\"tipe\": \"Formal (INMAD)\", \"indo\": row['Indonesia'], \"madura\": row['Madura']})\n",
    "            print(f\"‚úÖ Masuk: {len(sample)} soal dari INMAD.\")\n",
    "        except: print(\"‚ö†Ô∏è Gagal baca INMAD.\")\n",
    "    else:\n",
    "        print(f\"‚ùå File tidak ketemu: {PATH_INMAD}\")\n",
    "    \n",
    "    return pd.DataFrame(soal_ujian)\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ MULAI PENILAIAN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"\\nü§ñ MEMUAT MODEL DARI: {MODEL_PATH}\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"‚ö° Device: {device.upper()}\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå MODEL GAGAL DILOAD: {e}\")\n",
    "        print(\"Saran: Pastikan path foldernya benar dan sudah install protobuf.\")\n",
    "        return\n",
    "\n",
    "    df_test = load_data_ujian()\n",
    "    if len(df_test) == 0:\n",
    "        print(\"‚ùå Tidak ada soal ujian! Cek path file csv mu.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüöÄ MULAI UJIAN ({len(df_test)} Soal)...\")\n",
    "    print(\"Parameter: Beam=5, Normalisasi=AKTIF (Anti-Petik)\\n\")\n",
    "    \n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    preds, refs = [], []\n",
    "    \n",
    "    # Loop Evaluasi\n",
    "    for i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        indo = str(row['indo'])\n",
    "        kunci_raw = str(row['madura'])\n",
    "        \n",
    "        # Format Input (Wajib sama dengan training)\n",
    "        text_input = f\"terjemahkan dari Bahasa Indonesia ke Bahasa Madura: {indo}\"\n",
    "        inputs = tokenizer(text_input, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "        \n",
    "        # Generate Jawaban\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_length=128, \n",
    "                num_beams=5,             # Cari 5 kemungkinan terbaik\n",
    "                repetition_penalty=1.2,  # Jangan ngulang kata\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "        jawaban_model = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # --- PROSES PEMBERSIHAN (NORMALISASI) ---\n",
    "        jawaban_bersih = normalisasi_madura(jawaban_model)\n",
    "        kunci_bersih   = normalisasi_madura(kunci_raw)\n",
    "        # ----------------------------------------\n",
    "        \n",
    "        preds.append(jawaban_bersih)\n",
    "        refs.append([kunci_bersih])\n",
    "        \n",
    "        # Tampilkan contoh setiap 25 soal\n",
    "        if i % 25 == 0:\n",
    "            print(f\"\\n[{row['tipe']}] Indo: {indo}\")\n",
    "            print(f\"ü§ñ Jawab : {jawaban_model}  \\t-> (Norm: {jawaban_bersih})\")\n",
    "            print(f\"üîë Kunci : {kunci_raw}      \\t-> (Norm: {kunci_bersih})\")\n",
    "\n",
    "    # Hitung Skor Akhir\n",
    "    score = metric.compute(predictions=preds, references=refs)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"üèÜ SKOR BLEU FINAL (ADIL): {score['score']:.2f}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if score['score'] > 20:\n",
    "        print(\"‚úÖ Kualitas BAGUS! (Model paham makna, walau beda gaya)\")\n",
    "    elif score['score'] > 10:\n",
    "        print(\"‚ö†Ô∏è LUMAYAN. (Perlu perbaikan gaya bahasa)\")\n",
    "    else:\n",
    "        print(\"‚ùå RENDAH. (Mungkin data training terlalu sedikit/kotor)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
