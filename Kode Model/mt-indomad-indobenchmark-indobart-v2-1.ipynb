{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14135617,"sourceType":"datasetVersion","datasetId":9007649},{"sourceId":14137861,"sourceType":"datasetVersion","datasetId":9009334}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U indobenchmark-toolkit evaluate sacrebleu rouge-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U transformers accelerate evaluate sacrebleu rouge-score sentencepiece\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport os, re, unicodedata\nimport numpy as np\nimport pandas as pd\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\n\nimport evaluate\nprint(\"evaluate version:\", evaluate.__version__)\n\n# >>> ADDED: tokenizer khusus IndoBenchmark\nfrom indobenchmark import IndoNLGTokenizer","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:01:38.423580Z","iopub.status.busy":"2025-12-13T23:01:38.422974Z","iopub.status.idle":"2025-12-13T23:01:50.215964Z","shell.execute_reply":"2025-12-13T23:01:50.215146Z","shell.execute_reply.started":"2025-12-13T23:01:38.423551Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-12-13 23:01:44.049637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1765666904.071119     363 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1765666904.077576     363 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"]},{"name":"stdout","output_type":"stream","text":["evaluate version: 0.4.6\n"]}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport unicodedata\nimport numpy as np\n\nIN_PATH  = \"/kaggle/input/inmad-dataset/INMAD Dataset.csv\"\nOUT_PATH = \"inmad_clean_v2.csv\"\n\ndef fix_mojibake(s: str) -> str:\n    if not isinstance(s, str):\n        s = \"\" if s is None else str(s)\n\n    # heuristik sederhana: kalau ada √É/√Ç/ÔøΩ biasanya mojibake\n    if any(ch in s for ch in [\"√É\", \"√Ç\", \"ÔøΩ\", \"\\uFFFD\"]):\n        for src_enc in [\"latin-1\", \"cp1252\"]:\n            try:\n                s2 = s.encode(src_enc, errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n                if len(s2.strip()) > 0:\n                    s = s2\n                    break\n            except Exception:\n                pass\n    return s\n\ndef normalize_text(s: str) -> str:\n    s = fix_mojibake(s)\n    s = unicodedata.normalize(\"NFKC\", s)\n\n    # hapus control chars\n    s = re.sub(r\"[\\u0000-\\u001F\\u007F-\\u009F]\", \" \", s)\n    s = s.replace(\"\\u200b\", \" \").replace(\"\\ufeff\", \" \")\n\n    # normalisasi kutip/apostrof\n    s = (s.replace(\"‚Äô\",\"'\").replace(\"‚Äò\",\"'\").replace(\"¬¥\",\"'\").replace(\"`\",\"'\")\n           .replace(\"‚Äú\",'\"').replace(\"‚Äù\",'\"'))\n\n    # normalisasi dash dan ellipsis\n    s = s.replace(\"‚Äì\",\"-\").replace(\"‚Äî\",\"-\").replace(\"‚àí\",\"-\")\n    s = s.replace(\"‚Ä¶\",\"...\")\n\n    # rapikan spasi\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    # hilangkan spasi sebelum tanda baca: \" ,\", \" .\", dst\n    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n\n    # pastikan ada spasi setelah tanda baca jika langsung diikuti huruf/angka\n    s = re.sub(r\"([,;:!?])([A-Za-z0-9])\", r\"\\1 \\2\", s)\n    s = re.sub(r\"(\\.)([A-Za-z])\", r\"\\1 \\2\", s)  # \".kata\" -> \". kata\"\n\n    # rapikan kurung/bracket\n    s = re.sub(r\"\\(\\s+\", \"(\", s)\n    s = re.sub(r\"\\s+\\)\", \")\", s)\n    s = re.sub(r\"\\[\\s+\", \"[\", s)\n    s = re.sub(r\"\\s+\\]\", \"]\", s)\n\n    # collapse multi punctuation\n    s = re.sub(r\"([!?])\\1{1,}\", r\"\\1\", s)\n    s = re.sub(r\"\\.{4,}\", \"...\", s)\n\n    return s\n\ndef tok_len(s: str) -> int:\n    return len(re.findall(r\"\\S+\", str(s)))\n\n# ===== Load =====\nraw = pd.read_csv(IN_PATH)\n\n# Ambil kolom yang kita butuhkan: Indonesia & Madura (buang English)\ndf = raw.rename(columns={\"Indonesia\":\"id\", \"Madura\":\"mad\"}).copy()\ndf[\"id\"]  = df[\"id\"].astype(str).map(normalize_text)\ndf[\"mad\"] = df[\"mad\"].astype(str).map(normalize_text)\n\n# drop kosong + dedup\ndf = df[(df[\"id\"] != \"\") & (df[\"mad\"] != \"\")]\ndf = df.drop_duplicates(subset=[\"id\",\"mad\"]).reset_index(drop=True)\n\n# ===== Filter kualitas (biar tidak over-noisy) =====\nid_len  = df[\"id\"].map(tok_len)\nmad_len = df[\"mad\"].map(tok_len)\nratio   = (id_len + 1) / (mad_len + 1)\n\n# batas aman (kamu bisa adjust)\nkeep = (\n    (id_len  >= 3)  & (mad_len >= 3) &\n    (id_len  <= 200) & (mad_len <= 220) &\n    (ratio >= 0.5) & (ratio <= 2.0)\n)\n\ndf_clean = df[keep].reset_index(drop=True)\n\nprint(\"Raw rows:\", len(raw))\nprint(\"After basic clean:\", len(df))\nprint(\"After filter:\", len(df_clean))\n\n# ===== Save =====\ndf_clean[[\"id\",\"mad\"]].to_csv(OUT_PATH, index=False)\nprint(\"Saved:\", OUT_PATH)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-14T11:31:22.823907Z","iopub.execute_input":"2025-12-14T11:31:22.824803Z","iopub.status.idle":"2025-12-14T11:31:25.722913Z","shell.execute_reply.started":"2025-12-14T11:31:22.824766Z","shell.execute_reply":"2025-12-14T11:31:25.722116Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Raw rows: 23098\nAfter basic clean: 23032\nAfter filter: 21389\nSaved: inmad_clean_v2.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# MODEL\nMODEL_NAME = \"indobenchmark/indobart-v2\"\n","metadata":{"execution":{"iopub.status.busy":"2025-12-14T11:31:32.759587Z","iopub.execute_input":"2025-12-14T11:31:32.759909Z","iopub.status.idle":"2025-12-14T11:31:32.764367Z","shell.execute_reply.started":"2025-12-14T11:31:32.759887Z","shell.execute_reply":"2025-12-14T11:31:32.763417Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# prepro","metadata":{}},{"cell_type":"code","source":"def standardize_cols(df: pd.DataFrame) -> pd.DataFrame:\n    cols = {c.lower(): c for c in df.columns}\n    id_col  = cols.get(\"indonesian\") or cols.get(\"id\") or cols.get(\"indo\") or cols.get(\"source\")\n    mad_col = cols.get(\"madurese\") or cols.get(\"mad\") or cols.get(\"madura\") or cols.get(\"target\")\n    if id_col is None or mad_col is None:\n        raise ValueError(f\"Kolom id/mad tidak ketemu. Kolom yang ada: {list(df.columns)}\")\n    out = df[[id_col, mad_col]].copy()\n    out.columns = [\"id\", \"mad\"]\n    return out\n\ndef fix_mojibake(s: str) -> str:\n    if not isinstance(s, str):\n        s = \"\" if s is None else str(s)\n\n    # heuristik sederhana: kalau ada √É/√Ç/ÔøΩ biasanya mojibake\n    if any(ch in s for ch in [\"√É\", \"√Ç\", \"ÔøΩ\", \"\\uFFFD\"]):\n        for src_enc in [\"latin-1\", \"cp1252\"]:\n            try:\n                s2 = s.encode(src_enc, errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n                if len(s2.strip()) > 0:\n                    s = s2\n                    break\n            except Exception:\n                pass\n    return s\ndef clean_text(s: str) -> str:\n    s = fix_mojibake(s)\n    s = s.replace(\"\\u200b\", \" \").replace(\"\\ufeff\", \" \")\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef clean_df(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df[\"id\"]  = df[\"id\"].map(clean_text)\n    df[\"mad\"] = df[\"mad\"].map(clean_text)\n    df = df[(df[\"id\"] != \"\") & (df[\"mad\"] != \"\")]\n    df = df.drop_duplicates(subset=[\"id\",\"mad\"]).reset_index(drop=True)\n    return df\n\ndef drop_unnamed_cols(df: pd.DataFrame) -> pd.DataFrame:\n    unnamed = [c for c in df.columns if str(c).lower().startswith(\"unnamed\")]\n    if unnamed:\n        df = df.drop(columns=unnamed)\n    return df\n\ndef guess_column(df: pd.DataFrame, candidates):\n    cols_lower = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols_lower:\n            return cols_lower[cand.lower()]\n    return None","metadata":{"execution":{"iopub.status.busy":"2025-12-14T11:31:27.932629Z","iopub.execute_input":"2025-12-14T11:31:27.932936Z","iopub.status.idle":"2025-12-14T11:31:27.944742Z","shell.execute_reply.started":"2025-12-14T11:31:27.932913Z","shell.execute_reply":"2025-12-14T11:31:27.943672Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# import data","metadata":{}},{"cell_type":"code","source":"\nnusax_train = clean_df(standardize_cols(pd.read_csv(\"/kaggle/input/nusaxdata/train.csv\")))\nnusax_valid = clean_df(standardize_cols(pd.read_csv(\"/kaggle/input/nusaxdata/valid.csv\")))\nnusax_test  = clean_df(standardize_cols(pd.read_csv(\"/kaggle/input/nusaxdata/test (1).csv\")))  # sesuaikan nama file test kamu\n\nprint(len(nusax_train), len(nusax_valid), len(nusax_test))\n","metadata":{"execution":{"iopub.status.busy":"2025-12-14T11:31:38.255897Z","iopub.execute_input":"2025-12-14T11:31:38.256291Z","iopub.status.idle":"2025-12-14T11:31:38.344247Z","shell.execute_reply.started":"2025-12-14T11:31:38.256259Z","shell.execute_reply":"2025-12-14T11:31:38.343388Z"},"trusted":true},"outputs":[{"name":"stdout","text":"500 100 400\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"lex = pd.read_csv(\"/kaggle/input/nusaxdata/madurese.csv\")  # file lexicon\nlex = standardize_cols(lex)        # jadi id, mad\nlex = clean_df(lex)\n\n# bikin mapping mad->mad \"kanonik\" berbasis bentuk yang paling sering / paling pendek\n# (ini sederhana tapi efektif untuk merapikan variasi ejaan)\nmad2canon = {}\nfor _, r in lex.iterrows():\n    m = r[\"mad\"]\n    # pilih bentuk canon = bentuk yang \"paling clean\" (panjang paling pendek)\n    if m not in mad2canon:\n        mad2canon[m] = m\n\n# kalau kamu mau mapping variasi ke satu bentuk (misal bul√¢ vs bula'), kamu butuh aturan tambahan.\n# Untuk versi aman: kita pakai normalisasi karakter saja + perbaiki mojibake.\ndef normalize_madurese_with_lexicon(text: str) -> str:\n    # perbaiki encoding & rapikan spasi (yang paling aman)\n    return clean_text(text)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-14T11:31:39.887753Z","iopub.execute_input":"2025-12-14T11:31:39.888026Z","iopub.status.idle":"2025-12-14T11:31:39.961306Z","shell.execute_reply.started":"2025-12-14T11:31:39.888006Z","shell.execute_reply":"2025-12-14T11:31:39.960507Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"inmad = pd.read_csv(\"inmad_clean_v2.csv\")\n\n\n# normalisasi madurese pakai fungsi lexicon (safe)\ninmad[\"mad\"] = inmad[\"mad\"].map(normalize_madurese_with_lexicon)\n\nprint(\"inmad:\", len(inmad))\n","metadata":{"execution":{"iopub.status.busy":"2025-12-14T11:31:41.963693Z","iopub.execute_input":"2025-12-14T11:31:41.963989Z","iopub.status.idle":"2025-12-14T11:31:42.294821Z","shell.execute_reply.started":"2025-12-14T11:31:41.963965Z","shell.execute_reply":"2025-12-14T11:31:42.294116Z"},"trusted":true},"outputs":[{"name":"stdout","text":"inmad: 21389\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"rng = np.random.default_rng(42)\nidx = np.arange(len(inmad))\nrng.shuffle(idx)\n\nvalid_frac = 0.05   # 5% valid dari InMad\nn_valid = max(1, int(len(inmad) * valid_frac))\n\ninmad_valid = inmad.iloc[idx[:n_valid]].reset_index(drop=True)\ninmad_train = inmad.iloc[idx[n_valid:]].reset_index(drop=True)\n\n# (opsional) kalau InMad jauh lebih besar, batasi rasio biar NusaX nggak ketimbun\nmax_ratio = 3  # InMad train max 3x NusaX train\ntarget_inmad = min(len(inmad_train), max_ratio * len(nusax_train))\ninmad_train = inmad_train.sample(n=target_inmad, random_state=42).reset_index(drop=True)\n\n# tag sumber (optional tapi bagus buat kontrol domain)\nnusax_train[\"src\"] = \"nusax\"\nnusax_valid[\"src\"] = \"nusax\"\ninmad_train[\"src\"] = \"inmad\"\ninmad_valid[\"src\"] = \"inmad\"\n\ntrain_mix = pd.concat([nusax_train, inmad_train], ignore_index=True)\nvalid_mix = pd.concat([nusax_valid, inmad_valid], ignore_index=True)\n\nprint(\"train_mix:\", len(train_mix), \"valid_mix:\", len(valid_mix))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T11:31:43.626412Z","iopub.execute_input":"2025-12-14T11:31:43.626731Z","iopub.status.idle":"2025-12-14T11:31:43.646287Z","shell.execute_reply.started":"2025-12-14T11:31:43.626705Z","shell.execute_reply":"2025-12-14T11:31:43.645260Z"}},"outputs":[{"name":"stdout","text":"train_mix: 2000 valid_mix: 1169\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"pip install dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T11:32:41.203842Z","iopub.execute_input":"2025-12-14T11:32:41.204654Z","iopub.status.idle":"2025-12-14T11:32:49.199437Z","shell.execute_reply.started":"2025-12-14T11:32:41.204620Z","shell.execute_reply":"2025-12-14T11:32:49.198393Z"}},"outputs":[{"name":"stdout","text":"Collecting dataset\n  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\nCollecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from dataset) (1.17.1)\nCollecting banal>=1.0.1 (from dataset)\n  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=0.6.2->dataset) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=0.6.2->dataset) (4.15.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.2.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=0.6.2->dataset) (3.0.3)\nDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\nDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\nDownloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: banal, sqlalchemy, dataset\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.41\n    Uninstalling SQLAlchemy-2.0.41:\n      Successfully uninstalled SQLAlchemy-2.0.41\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.18.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.54\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def build_bidirectional(df: pd.DataFrame) -> Dataset:\n    rows = []\n    for _, r in df.iterrows():\n        rows.append({\n            \"direction\": \"id2mad\",\n            \"source\": \"translate Indonesian to Madurese: \" + r[\"id\"],\n            \"target\": r[\"mad\"]\n        })\n        rows.append({\n            \"direction\": \"mad2id\",\n            \"source\": \"translate Madurese to Indonesian: \" + r[\"mad\"],\n            \"target\": r[\"id\"]\n        })\n    return Dataset.from_pandas(pd.DataFrame(rows))\n\ntrain_ds = build_bidirectional(train_mix)\nvalid_ds = build_bidirectional(valid_mix)\ntest_ds  = build_bidirectional(nusax_test)\n\ntrain_ds[0], train_ds[1]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# model","metadata":{}},{"cell_type":"code","source":"from indobenchmark import IndoNLGTokenizer\ntokenizer = IndoNLGTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:06:26.419388Z","iopub.status.busy":"2025-12-13T23:06:26.418993Z","iopub.status.idle":"2025-12-13T23:06:27.620087Z","shell.execute_reply":"2025-12-13T23:06:27.619498Z","shell.execute_reply.started":"2025-12-13T23:06:26.419363Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# >>> ADDED: patch supaya IndoNLGTokenizer.pad() kompatibel dengan transformers baru\n_orig_pad = tokenizer.pad\n\ndef _pad_compat(encoded_inputs, *args, **kwargs):\n    # buang argumen yang bikin crash di IndoNLGTokenizer versi lama\n    kwargs.pop(\"padding_side\", None)\n    kwargs.pop(\"return_tensor\", None)  # kadang typo lama/beda nama\n    return _orig_pad(encoded_inputs, *args, **kwargs)\n\ntokenizer.pad = _pad_compat\n","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:06:32.445338Z","iopub.status.busy":"2025-12-13T23:06:32.445013Z","iopub.status.idle":"2025-12-13T23:06:32.449718Z","shell.execute_reply":"2025-12-13T23:06:32.449064Z","shell.execute_reply.started":"2025-12-13T23:06:32.445316Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"MAX_LEN_SRC = 128\nMAX_LEN_TGT = 128\n\ndef tokenize_batch(batch):\n    inputs = tokenizer(\n        batch[\"source\"],\n        truncation=True,\n        max_length=MAX_LEN_SRC\n    )\n\n    labels = tokenizer(\n        batch[\"target\"],\n        truncation=True,\n        max_length=MAX_LEN_TGT\n    )\n\n    inputs[\"labels\"] = labels[\"input_ids\"]\n    return inputs\n\ntrain_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=train_ds.column_names)\nvalid_tok = valid_ds.map(tokenize_batch, batched=True, remove_columns=valid_ds.column_names)\ntest_tok  = test_ds.map(tokenize_batch,  batched=True, remove_columns=test_ds.column_names)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    label_pad_token_id=-100\n)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:06:40.452245Z","iopub.status.busy":"2025-12-13T23:06:40.451739Z","iopub.status.idle":"2025-12-13T23:06:41.447068Z","shell.execute_reply":"2025-12-13T23:06:41.446413Z","shell.execute_reply.started":"2025-12-13T23:06:40.452221Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5058b8fa82745a58f84a72cf7509edc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f40fd27393d1477fbdb54c0f0dd2da90","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7691db2b0394ba5bafbc530ac80d760","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/800 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":18},{"cell_type":"code","source":"bleu = evaluate.load(\"sacrebleu\")\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # >>> ADDED: preds kadang tuple\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    bleu_score = bleu.compute(predictions=pred_texts, references=[[r] for r in ref_texts])[\"score\"]\n    rouge_score = rouge.compute(predictions=pred_texts, references=ref_texts)\n\n    return {\n        \"bleu\": bleu_score,\n        \"rouge1\": rouge_score[\"rouge1\"],\n        \"rougeL\": rouge_score[\"rougeL\"]\n    }\n","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:06:51.890954Z","iopub.status.busy":"2025-12-13T23:06:51.890663Z","iopub.status.idle":"2025-12-13T23:06:53.739493Z","shell.execute_reply":"2025-12-13T23:06:53.738883Z","shell.execute_reply.started":"2025-12-13T23:06:51.890933Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63286e2f2d7242a7b980e0a31eee4d41","version_major":2,"version_minor":0},"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50feeb0734a544d8ac8a56e76fa14326","version_major":2,"version_minor":0},"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":19},{"cell_type":"code","source":"import transformers, sys\nprint(\"transformers version:\", transformers.__version__)\nprint(\"python:\", sys.version)\n\nOUTPUT_DIR = \"./indobenchmark-indobart-v2\"\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    save_steps=10**9,        # praktis tidak pernah save checkpoint\n    save_total_limit=1,\n    logging_steps=100,\n    learning_rate=3e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=1,\n    num_train_epochs=10,\n    fp16=True,\n    report_to=\"none\",\n    prediction_loss_only=True,\n)","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:19:34.474938Z","iopub.status.busy":"2025-12-13T23:19:34.474637Z","iopub.status.idle":"2025-12-13T23:19:34.508778Z","shell.execute_reply":"2025-12-13T23:19:34.508138Z","shell.execute_reply.started":"2025-12-13T23:19:34.474915Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["transformers version: 4.57.3\n","python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n"]}],"execution_count":24},{"cell_type":"code","source":"import transformers, sys\nprint(\"transformers version:\", transformers.__version__)\nprint(\"python:\", sys.version)\n\nOUTPUT_DIR = \"./indobenchmark-indobart-v2\"\n\n# ======================\n# 1) PATCH: cegah autosave tokenizer (IndoNLGTokenizer tidak support save_vocabulary)\n# ======================\ndef _noop_save_pretrained(*args, **kwargs):\n    return ()\n\ntokenizer.save_pretrained = _noop_save_pretrained\ntokenizer.save_vocabulary = lambda *args, **kwargs: ()\n\n# ======================\n# 2) TrainingArguments (aman lintas versi)\n# ======================\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    save_steps=10**9,         # praktis tidak pernah save checkpoint\n    save_total_limit=1,\n    logging_steps=100,\n\n    learning_rate=3e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=1,\n    num_train_epochs=10,\n\n    fp16=True,\n    report_to=\"none\",\n    prediction_loss_only=True,\n)\n\n# ======================\n# 3) BLEU per epoch callback + AVG + summary\n# ======================\nimport torch\nimport sacrebleu\nfrom transformers import TrainerCallback\n\nBLEU_LOG = {}  # {epoch_int: {\"id2mad\":..., \"mad2id\":..., \"avg\":...}}\n\ndef _epoch_key(state):\n    if state.epoch is None:\n        return int(getattr(state, \"global_step\", 0))\n    return int(state.epoch)\n\nclass BleuEachEpochCallback(TrainerCallback):\n    def __init__(self, tokenizer, valid_df, direction=\"id2mad\", n_samples=100,\n                 max_len_src=128, max_new_tok=128, batch_size=8, num_beams=4):\n        self.tokenizer = tokenizer\n        self.valid_df = valid_df\n        self.direction = direction\n        self.n_samples = n_samples\n        self.max_len_src = max_len_src\n        self.max_new_tok = max_new_tok\n        self.batch_size = batch_size\n        self.num_beams = num_beams\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        model = kwargs[\"model\"]\n        device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.eval()\n\n        df = self.valid_df.head(self.n_samples)\n\n        if self.direction == \"id2mad\":\n            sources = [\"translate Indonesian to Madurese: \" + x for x in df[\"id\"].tolist()]\n            refs = df[\"mad\"].tolist()\n        else:\n            sources = [\"translate Madurese to Indonesian: \" + x for x in df[\"mad\"].tolist()]\n            refs = df[\"id\"].tolist()\n\n        preds = []\n        with torch.no_grad():\n            for i in range(0, len(sources), self.batch_size):\n                batch = sources[i:i+self.batch_size]\n                enc = self.tokenizer(\n                    batch,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    truncation=True,\n                    max_length=self.max_len_src\n                )\n                enc = {k: v.to(device) for k, v in enc.items()}\n                out = model.generate(**enc, max_new_tokens=self.max_new_tok, num_beams=self.num_beams)\n\n                # FIX: IndoNLGTokenizer.decode tidak support kwargs dari batch_decode\n                preds.extend([self.tokenizer.decode(o, skip_special_tokens=True) for o in out])\n\n        bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n\n        ep = _epoch_key(state)\n        BLEU_LOG.setdefault(ep, {})\n        BLEU_LOG[ep][self.direction] = bleu\n\n        # print avg jika dua arah sudah ada\n        if \"id2mad\" in BLEU_LOG[ep] and \"mad2id\" in BLEU_LOG[ep]:\n            avg_bleu = (BLEU_LOG[ep][\"id2mad\"] + BLEU_LOG[ep][\"mad2id\"]) / 2.0\n            BLEU_LOG[ep][\"avg\"] = avg_bleu\n            print(\n                f\"\\nüèÜ Epoch {ep} | ID2MAD BLEU@{self.n_samples}: {BLEU_LOG[ep]['id2mad']:.2f} | \"\n                f\"MAD2ID BLEU@{self.n_samples}: {BLEU_LOG[ep]['mad2id']:.2f} | \"\n                f\"AVG: {avg_bleu:.2f}\\n\"\n            )\n        else:\n            print(f\"\\nüèÜ Epoch {ep} | {self.direction.upper()} BLEU@{self.n_samples}: {bleu:.2f}\\n\")\n\n        model.train()\n        return control\n\nclass BleuAvgSummaryCallback(TrainerCallback):\n    def on_train_end(self, args, state, control, **kwargs):\n        if not BLEU_LOG:\n            print(\"\\n‚ö†Ô∏è BLEU_LOG kosong (tidak ada BLEU yang tercatat)\\n\")\n            return control\n\n        print(\"\\n==============================\")\n        print(\"üìå RINGKASAN BLEU PER EPOCH (AVG)\")\n        print(\"==============================\")\n        for ep in sorted(BLEU_LOG.keys()):\n            rec = BLEU_LOG[ep]\n            id2 = rec.get(\"id2mad\", float(\"nan\"))\n            m2i = rec.get(\"mad2id\", float(\"nan\"))\n            avg = rec.get(\"avg\", float(\"nan\"))\n            print(f\"Epoch {ep}: ID2MAD={id2:.2f} | MAD2ID={m2i:.2f} | AVG={avg:.2f}\")\n\n        avgs = [BLEU_LOG[ep][\"avg\"] for ep in sorted(BLEU_LOG.keys()) if \"avg\" in BLEU_LOG[ep]]\n        if avgs:\n            overall = sum(avgs) / len(avgs)\n            print(\"------------------------------\")\n            print(f\"‚úÖ Overall AVG BLEU across epochs: {overall:.2f}\")\n        print(\"==============================\\n\")\n        return control\n\n# buat callback dua arah + summary\nbleu_cb_id2mad = BleuEachEpochCallback(\n    tokenizer=tokenizer,\n    valid_df=valid_mix,\n    direction=\"id2mad\",\n    n_samples=100,\n    max_len_src=128,\n    max_new_tok=128,\n    batch_size=8,\n    num_beams=4\n)\n\nbleu_cb_mad2id = BleuEachEpochCallback(\n    tokenizer=tokenizer,\n    valid_df=valid_mix,\n    direction=\"mad2id\",\n    n_samples=100,\n    max_len_src=128,\n    max_new_tok=128,\n    batch_size=8,\n    num_beams=4\n)\n\nbleu_summary = BleuAvgSummaryCallback()\n\n# ======================\n# 4) Trainer + train\n# ======================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=valid_tok,\n    data_collator=data_collator,\n    callbacks=[bleu_cb_id2mad, bleu_cb_mad2id, bleu_summary]\n)\n\ntrainer.train()\n\n# ======================\n# 5) Save model (tokenizer tidak disave)\n# ======================\ntrainer.save_model(OUTPUT_DIR)\nprint(\"‚úÖ Model disimpan ke:\", OUTPUT_DIR)\nprint(\"‚ÑπÔ∏è Tokenizer tidak disimpan (pakai tokenizer bawaan indobenchmark/indobart-v2).\")\n\nbest_ckpt = getattr(trainer.state, \"best_model_checkpoint\", None)\nbest_metric = getattr(trainer.state, \"best_metric\", None)\nprint(\"Best checkpoint:\", best_ckpt)\nprint(\"Best metric:\", best_metric)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-13T23:47:07.076918Z","iopub.status.busy":"2025-12-13T23:47:07.076616Z","iopub.status.idle":"2025-12-14T00:00:27.001754Z","shell.execute_reply":"2025-12-14T00:00:27.001110Z","shell.execute_reply.started":"2025-12-13T23:47:07.076898Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You are adding a <class '__main__.BleuEachEpochCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",":DefaultFlowCallback\n","BleuEachEpochCallback\n"]},{"name":"stdout","output_type":"stream","text":["transformers version: 4.57.3\n","python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1250/1250 13:18, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.018500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.032000</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.033400</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.031600</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.026200</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.029100</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.024300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.021600</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.021200</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.021200</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.023700</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.029300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","üèÜ Epoch 1 | ID2MAD BLEU@100: 8.08\n","\n","\n","üèÜ Epoch 1 | ID2MAD BLEU@100: 8.08 | MAD2ID BLEU@100: 17.94 | AVG: 13.01\n","\n","\n","üèÜ Epoch 2 | ID2MAD BLEU@100: 8.50\n","\n","\n","üèÜ Epoch 2 | ID2MAD BLEU@100: 8.50 | MAD2ID BLEU@100: 16.89 | AVG: 12.70\n","\n","\n","üèÜ Epoch 3 | ID2MAD BLEU@100: 7.57\n","\n","\n","üèÜ Epoch 3 | ID2MAD BLEU@100: 7.57 | MAD2ID BLEU@100: 14.34 | AVG: 10.96\n","\n","\n","üèÜ Epoch 4 | ID2MAD BLEU@100: 8.79\n","\n","\n","üèÜ Epoch 4 | ID2MAD BLEU@100: 8.79 | MAD2ID BLEU@100: 15.56 | AVG: 12.18\n","\n","\n","üèÜ Epoch 5 | ID2MAD BLEU@100: 8.64\n","\n","\n","üèÜ Epoch 5 | ID2MAD BLEU@100: 8.64 | MAD2ID BLEU@100: 14.98 | AVG: 11.81\n","\n","\n","üèÜ Epoch 6 | ID2MAD BLEU@100: 8.65\n","\n","\n","üèÜ Epoch 6 | ID2MAD BLEU@100: 8.65 | MAD2ID BLEU@100: 14.83 | AVG: 11.74\n","\n","\n","üèÜ Epoch 7 | ID2MAD BLEU@100: 8.08\n","\n","\n","üèÜ Epoch 7 | ID2MAD BLEU@100: 8.08 | MAD2ID BLEU@100: 14.21 | AVG: 11.15\n","\n","\n","üèÜ Epoch 8 | ID2MAD BLEU@100: 9.01\n","\n","\n","üèÜ Epoch 8 | ID2MAD BLEU@100: 9.01 | MAD2ID BLEU@100: 13.65 | AVG: 11.33\n","\n","\n","üèÜ Epoch 9 | ID2MAD BLEU@100: 8.61\n","\n","\n","üèÜ Epoch 9 | ID2MAD BLEU@100: 8.61 | MAD2ID BLEU@100: 14.57 | AVG: 11.59\n","\n","\n","üèÜ Epoch 10 | ID2MAD BLEU@100: 8.52\n","\n","\n","üèÜ Epoch 10 | ID2MAD BLEU@100: 8.52 | MAD2ID BLEU@100: 14.10 | AVG: 11.31\n","\n","\n","==============================\n","üìå RINGKASAN BLEU PER EPOCH (AVG)\n","==============================\n","Epoch 1: ID2MAD=8.08 | MAD2ID=17.94 | AVG=13.01\n","Epoch 2: ID2MAD=8.50 | MAD2ID=16.89 | AVG=12.70\n","Epoch 3: ID2MAD=7.57 | MAD2ID=14.34 | AVG=10.96\n","Epoch 4: ID2MAD=8.79 | MAD2ID=15.56 | AVG=12.18\n","Epoch 5: ID2MAD=8.64 | MAD2ID=14.98 | AVG=11.81\n","Epoch 6: ID2MAD=8.65 | MAD2ID=14.83 | AVG=11.74\n","Epoch 7: ID2MAD=8.08 | MAD2ID=14.21 | AVG=11.15\n","Epoch 8: ID2MAD=9.01 | MAD2ID=13.65 | AVG=11.33\n","Epoch 9: ID2MAD=8.61 | MAD2ID=14.57 | AVG=11.59\n","Epoch 10: ID2MAD=8.52 | MAD2ID=14.10 | AVG=11.31\n","------------------------------\n","‚úÖ Overall AVG BLEU across epochs: 11.78\n","==============================\n","\n","‚úÖ Model disimpan ke: ./indobenchmark-indobart-v2\n","‚ÑπÔ∏è Tokenizer tidak disimpan (pakai tokenizer bawaan indobenchmark/indobart-v2).\n","Best checkpoint: None\n","Best metric: None\n"]}],"execution_count":null},{"cell_type":"code","source":"try:\n    tokenizer.save_pretrained(OUTPUT_DIR)\nexcept Exception as e:\n    print(\"Tokenizer tidak bisa disave dengan save_pretrained (aman di-skip):\", repr(e))","metadata":{"execution":{"iopub.execute_input":"2025-12-14T00:01:40.643322Z","iopub.status.busy":"2025-12-14T00:01:40.643038Z","iopub.status.idle":"2025-12-14T00:01:40.647372Z","shell.execute_reply":"2025-12-14T00:01:40.646624Z","shell.execute_reply.started":"2025-12-14T00:01:40.643302Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# ======================\n# CUSTOM EVALUATION (PRINT + BLEU + CONTOH)\n# ======================\n\nfrom tqdm.auto import tqdm\nimport sacrebleu\nimport torch\n\nprint(\"üìÇ Memuat model...\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = trainer.model.to(device)\nmodel.eval()\n\nTEST_PATH = \"/kaggle/input/nusaxdata/test (1).csv\"  \n\nprint(f\"üìÇ Membaca data tes: {TEST_PATH}\")\ntest_df = pd.read_csv(TEST_PATH)\ntest_df = drop_unnamed_cols(test_df)\n\nid_col = guess_column(test_df, [\"ind\", \"id\", \"indo\", \"indonesian\"])\nmad_col = guess_column(test_df, [\"mad\", \"madurese\", \"madura\"])\ntest_df = test_df[[id_col, mad_col]].rename(columns={id_col: \"id\", mad_col: \"mad\"})\n\ntest_df[\"id\"]  = test_df[\"id\"].apply(clean_text)\ntest_df[\"mad\"] = test_df[\"mad\"].apply(clean_text)\n\nN = 100\ntest_df = test_df.head(N)\nprint(f\"‚úÖ Menguji pada {len(test_df)} kalimat pertama.\")\nprint(\"üöÄ Mulai Menerjemahkan...\")\n\nsources = [\"translate Indonesian to Madurese: \" + x for x in test_df[\"id\"].tolist()]\nrefs    = test_df[\"mad\"].tolist()\n\npreds = []\nbatch_size = 8\n\nfor i in tqdm(range(0, len(sources), batch_size)):\n    batch = sources[i:i+batch_size]\n    enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    enc = {k: v.to(device) for k, v in enc.items()}\n    with torch.no_grad():\n        out = model.generate(**enc, max_new_tokens=128, num_beams=4)\n\n    # >>> FIX DI SINI\n    preds.extend([tokenizer.decode(o, skip_special_tokens=True) for o in out])\n\nbleu = sacrebleu.corpus_bleu(preds, [refs]).score\n\nprint(\"\\n==============================\")\nprint(f\"üèÜ REAL BLEU SCORE: {bleu:.2f}\")\nprint(\"==============================\\n\")\n\nprint(\"üîç 5 CONTOH HASIL:\")\nfor i in range(min(5, len(test_df))):\n    print(f\"üáÆüá© Indo  : {test_df.iloc[i]['id']}\")\n    print(f\"ü§ñ Model : {preds[i]}\")\n    print(f\"üîë Kunci : {refs[i]}\")\n    print(\"-\" * 20)\n","metadata":{"execution":{"iopub.execute_input":"2025-12-14T00:02:40.492266Z","iopub.status.busy":"2025-12-14T00:02:40.491962Z","iopub.status.idle":"2025-12-14T00:03:05.134821Z","shell.execute_reply":"2025-12-14T00:03:05.134034Z","shell.execute_reply.started":"2025-12-14T00:02:40.492245Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["üìÇ Memuat model...\n","üìÇ Membaca data tes: /kaggle/working/nusax/test.csv\n","‚úÖ Menguji pada 100 kalimat pertama.\n","üöÄ Mulai Menerjemahkan...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8eb48d9a905b4230bbf7f095444eddc9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/13 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","==============================\n","üèÜ REAL BLEU SCORE: 8.81\n","==============================\n","\n","üîç 5 CONTOH HASIL:\n","üáÆüá© Indo  : Dekat dengan hotel saya menginap, hanya ditempuh jalan kaki, di sini banyak sekali pilihan makanannya, tempat yang luas, dan menyenangkan\n","ü§ñ Model :  semma' bik hotel engkok nginep, pera' ditempuh jelen kaki, e diye bennyak sarah pelean kakananna, kennengngan se leber, ben masenneng sarah. aminnih. entara bik hotel riya. bhuktena se lebbi mude... tapegha. bannya' sarah.. bhaghus.. ban mapegghel.. bhekal..ghau..ha..ah..adeh..kadherse.adek..egghu.regghu bik hotel.raddhuk\n","üîë Kunci : Semmak bik hotel engkok nginep, pera' ejeleni ajelen soko, ediye bennyak sarah pelean kakananna, kenengngan se leber, ben masenneng\n","--------------------\n","üáÆüá© Indo  : Iya benar, dia sedang jaga warung.\n","ü§ñ Model :  iye bhender, engkok bik selaen jaga warung. bhenderre' sarah, bekna bherseh.'. bhuktena bhender.'egus..'raddhis.' bhuk.'ade'.. bhekallis.'adeh..raddhuk. bhegusse..regghu..ajherse.ajhek.ajraddhik.. deddhi bhegghik.ajik.dhik engkok bhegik.regik.bi'.ajhuk.ajajhek engkok iye bheguk.\n","üîë Kunci : Iye bhender, rua ajege berung.\n","--------------------\n","üáÆüá© Indo  : Kangkungnya lumayan tapi kepiting saus padangnya mengecewakan kami dikasih kepiting yang kopong akhir kami tidak makan keptingnya dan dikembalikan.\n","ü§ñ Model :  kakananna pendhenan tape kopong saus padang na tak masenneng engkok bik selaen eberrik kopong se kopong akhirna engkok ngakan keptingnga ben epanengngenneng. bhuktena jhek ta' ngakan ghenganna ben ebhengannengngi.ghi.gui.gha.ha. ba'na. banni'. bhingnga'.ha'. bhaghusse.gghus.nga',dhi'egus.dhi kangkung. tape'egghus kiya.nga..g\n","üîë Kunci : Kangkongnga pendhanan tape kopeteng saos padangnga ma kocaba, engko' bi' laenna e bharri' kopeteng se kopong akherra engko' bi' laenna ta' ngakan kopeteng ban e pabali.\n","--------------------\n","üáÆüá© Indo  : Bertempat di braga city walk yang satu gedung dengan aston dan fave hotel, tempat ini sangat nyaman buat kongkow-kongkow. Kopi campur teh yang baru pertama kali saya nikmati ternyata sangat enak, dipadu dengan telur setengah matang menjadi pendamping mengobrol bersama teman-teman. Area yang bebas merokok semakin mengasyikkan sambil menikmati pemandangan lalu lalang orang-orang yang keluar masuk mal ini.\n","ü§ñ Model :  taghien e braga city walk se settong gedung bik aston ben fave hotel, kennengngan riya nyaman ghebey apolkompol. biddheng ghuring teh se bhuru pertama kale engkok nikmati bhegus sarah, e cellep bik telur setengah matang deddhi pendamping ngakan bik cakanca. area se bebas merokok saengghe atambe sambi menikmati pangabesen jejen-jejen se esadiye'eghi ebejer mal riya. kennengngenna se lerres ongghu ebejer'e torcatoran abit riya.. bhendinga\n","üîë Kunci : Kenengnganna e braga city walk se settong gheddung bik aston ben fave hotel, kenengngan riya nyaman sarah ghebey tor-catoran. Biddheng campor teh se ludhulluna engkok nginum bhuktena cek nyamanna, ecampor bik tellor massak saparo deddhi pendamping tor-catoran bik ca-kanca. Kenengngan se olle arokok tambe masenneng sambi menikmati pangabesen oreng-oreng se aje'genjir kaluar masok mal riya.\n","--------------------\n","üáÆüá© Indo  : Gianyar terima bantuan sosial 2018 sebesar rp 44, 9 miliar\n","ü§ñ Model :  gianyar kaso'paghi bantuan sosial 2018 ongghu rp 44, 9 miliar. bhegus sarah.ghi. gianyar.dhe.selengkapnya. bhehus.geh.gha.gherse.gghe.bheh.beh.ceh.deh.eh..bhe.eheng.begghel.geggh.ghek.eh!eh.ugeh.regghu.ugghek.eheh.edeh.eggheh. ekan.ehendeh.radeh.ek.ed.eh cokop\n","üîë Kunci : Gianyar tarema bhantoan sosial 2018 saraja rp 44,9 miliar.\n","--------------------\n"]}],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = trainer.model.to(device)\nmodel.eval()\n\nMAX_LEN_SRC = 128\nMAX_NEW_TOK = 128\n\ndef generate_batch(sources, batch_size=8):\n    preds = []\n    for i in range(0, len(sources), batch_size):\n        batch = sources[i:i+batch_size]\n        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN_SRC)\n        enc = {k: v.to(device) for k, v in enc.items()}\n        with torch.no_grad():\n            out = model.generate(**enc, max_new_tokens=MAX_NEW_TOK, num_beams=4)\n\n        # >>> CHANGED: jangan batch_decode untuk IndoNLGTokenizer\n        preds.extend([tokenizer.decode(o, skip_special_tokens=True) for o in out])\n\n    return preds\n\nimport evaluate\nbleu = evaluate.load(\"sacrebleu\")\nrouge = evaluate.load(\"rouge\")\n\ndef score(preds, refs):\n    b = bleu.compute(predictions=preds, references=[[r] for r in refs])[\"score\"]\n    r = rouge.compute(predictions=preds, references=refs)\n    return {\"BLEU\": b, \"ROUGE-1\": r[\"rouge1\"], \"ROUGE-L\": r[\"rougeL\"]}\n\n# VALID\nsrc_id2mad = [\"translate Indonesian to Madurese: \" + x for x in valid_clean[\"id\"].tolist()]\nref_id2mad = valid_mix[\"mad\"].tolist()\npred_id2mad = generate_batch(src_id2mad)\nprint(\"VALID ID ‚Üí MAD:\", score(pred_id2mad, ref_id2mad))\n\nsrc_mad2id = [\"translate Madurese to Indonesian: \" + x for x in valid_clean[\"mad\"].tolist()]\nref_mad2id = valid_mix[\"id\"].tolist()\npred_mad2id = generate_batch(src_mad2id)\nprint(\"VALID MAD ‚Üí ID:\", score(pred_mad2id, ref_mad2id))\n\n# TEST\nsrc_id2mad = [\"translate Indonesian to Madurese: \" + x for x in test_clean[\"id\"].tolist()]\nref_id2mad = nusax_test[\"mad\"].tolist()\npred_id2mad = generate_batch(src_id2mad)\nprint(\"TEST ID ‚Üí MAD:\", score(pred_id2mad, ref_id2mad))\n\nsrc_mad2id = [\"translate Madurese to Indonesian: \" + x for x in test_clean[\"mad\"].tolist()]\nref_mad2id = nusax_test[\"id\"].tolist()\npred_mad2id = generate_batch(src_mad2id)\nprint(\"TEST MAD ‚Üí ID:\", score(pred_mad2id, ref_mad2id))\n","metadata":{"execution":{"iopub.execute_input":"2025-12-14T00:07:00.038895Z","iopub.status.busy":"2025-12-14T00:07:00.038183Z","iopub.status.idle":"2025-12-14T00:11:03.521173Z","shell.execute_reply":"2025-12-14T00:11:03.520451Z","shell.execute_reply.started":"2025-12-14T00:07:00.038872Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["VALID ID ‚Üí MAD: {'BLEU': 8.52114696833872, 'ROUGE-1': 0.3320086378972633, 'ROUGE-L': 0.3141487554458656}\n","VALID MAD ‚Üí ID: {'BLEU': 14.100656809525388, 'ROUGE-1': 0.45444854182740113, 'ROUGE-L': 0.4439560797590716}\n","TEST ID ‚Üí MAD: {'BLEU': 9.082167730131204, 'ROUGE-1': 0.3410521121564547, 'ROUGE-L': 0.32573339839176685}\n","TEST MAD ‚Üí ID: {'BLEU': 14.444095656747404, 'ROUGE-1': 0.4696922027914887, 'ROUGE-L': 0.45547109612726866}\n"]}],"execution_count":null},{"cell_type":"code","source":"def translate(text: str, direction=\"id2mad\"):\n    text = text.strip()\n    if direction == \"id2mad\":\n        src = \"translate Indonesian to Madurese: \" + text\n    elif direction == \"mad2id\":\n        src = \"translate Madurese to Indonesian: \" + text\n    else:\n        raise ValueError(\"direction harus 'id2mad' atau 'mad2id'\")\n\n    enc = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN_SRC).to(device)\n    with torch.no_grad():\n        out = model.generate(**enc, max_new_tokens=MAX_NEW_TOK, num_beams=4)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\nwhile True:\n    direction = input(\"\\nPilih arah (id2mad / mad2id) atau ketik q: \").strip()\n    if direction.lower() == \"q\":\n        break\n    text = input(\"Masukkan teks: \").strip()\n    print(\"Hasil:\", translate(text, direction=direction))\n","metadata":{"execution":{"iopub.execute_input":"2025-12-14T00:12:26.332240Z","iopub.status.busy":"2025-12-14T00:12:26.331505Z","iopub.status.idle":"2025-12-14T00:13:32.147130Z","shell.execute_reply":"2025-12-14T00:13:32.146248Z","shell.execute_reply.started":"2025-12-14T00:12:26.332184Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Pilih arah (id2mad / mad2id) atau ketik q:  id2mad\n","Masukkan teks:  aku mau makan nasi goreng\n"]},{"name":"stdout","output_type":"stream","text":["Hasil:  engkok terro ngakan nasek ghuring ghuringnga. bhuktena engkok bhender.gggh.gghu.gha. bhallis.bbhina.bhina engkok.ggha.bherse.bhei.beh.begghu engkok bik bekna.bik.gik.regghu bi' bi'.reghi engkok ghik.bi'.bi...bigus..regik.. engkokregghuk.. lebbi engkok lebbi nyaman..raddhuk.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Pilih arah (id2mad / mad2id) atau ketik q:  mad2id\n","Masukkan teks:  engkok terro ngakan nasek ghuring\n"]},{"name":"stdout","output_type":"stream","text":["Hasil:  saya ingin makan nasi goreng pedas. tidak rekomendasi. tidak direkomendasikan. tidakenak.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Pilih arah (id2mad / mad2id) atau ketik q:  q\n"]}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}